#projet 2nd semestre

# ğŸ§® Probability Measure Sampling Optimization

A mathematical and computational project conducted at **Ã‰cole des Ponts ParisTech** and **INRIA Paris**, focused on improving the **sampling of probability measures** using **Markov Chain Monte Carlo (MCMC)** methods and **finite elementâ€“based optimization**.

ğŸ“˜ **Authors:**  
Adle Ben Salem, Maryam El Yaagoubi, Arnaud Masseron, Maxime Muhlethaler  
ğŸ‘¨â€ğŸ« **Supervisor:** RÃ©gis Santet (CERMICS / INRIA Paris)

ğŸ“ GitHub Repository: [https://github.com/MaximeMuh/PROJET](https://github.com/MaximeMuh/PROJET)

---

## ğŸ¯ Project Overview

This project explores how to **sample complex probability measures** efficiently â€” a central problem in **Bayesian inference**, **image generation**, and **statistical physics**.  

Traditional methods (e.g., inverse transform or rejection sampling) quickly become intractable in high dimensions.  
We therefore studied and implemented **Markov Chain Monte Carlo (MCMC)** algorithms and a novel **finite-element optimization approach** to accelerate convergence.

---

## ğŸ§© Objectives

1. **Sample a probability measure** without computing its normalization constant.  
2. **Optimize the sampling process** to reduce the number of iterations and improve convergence.

---

## ğŸ”¬ Theoretical Foundations

### ğŸ§  Markov Chains
The project begins with a rigorous study of **Markov chains**, defining key properties such as:
- Transition matrices and stochastic processes  
- Stationary and reversible distributions  
- Irreducibility, positive recurrence, and aperiodicity  
- The **Ergodic Theorem**, ensuring convergence of empirical averages toward expectations under the stationary measure

An illustrative example of a **3-state Markov chain** (A, B, C exchanging a ball) demonstrates convergence to a unique stationary distribution.

---

## ğŸ² MCMC Methods

We implemented and compared several **sampling algorithms**:

### 1ï¸âƒ£ Rejection Sampling
A simple baseline for generating samples from a target distribution using a proposal envelope.  
âœ… Efficient in low dimension  
âš ï¸ Computationally expensive in higher dimensions (high rejection rate)

### 2ï¸âƒ£ Metropolisâ€“Hastings (MH)
A **random walk**â€“based algorithm generating a Markov chain whose stationary distribution matches the target measure.

**Principle:**  
Each new candidate sample is accepted or rejected based on the acceptance ratio  
\[
\alpha = \min\left(1, \frac{\pi(q_{n+1})}{\pi(q_n)}\right)
\]

**Pros:** No need for normalization constants  
**Cons:** Sensitive to step size (Î”t) and dimensionality

### 3ï¸âƒ£ Metropolis Adjusted Langevin Algorithm (MALA)
An advanced variant of MH using **gradient information** of the target log-density:

\[
q_{n+1} = q_n + \Delta t \, \nabla \log \pi(q_n) + \sqrt{2 \Delta t} \, \xi_n
\]

This accelerates convergence by guiding proposals toward high-probability regions.

âœ… Faster convergence  
âš ï¸ Requires differentiable target and gradient computations

---

## âš™ï¸ Sampling Optimization with Finite Elements

To further **accelerate convergence**, the project introduces a **finite-element method (FEM)**â€“based optimization of the diffusion coefficient \( D(x) \) in the **Langevin equation**:

\[
\frac{\partial X}{\partial t} = -D(X)\nabla V(X) + \nabla \cdot (D(X)) + \sqrt{2 D(X)} \frac{\partial W}{\partial t}
\]

### ğŸ§© Finite Element Discretization
- The Fokkerâ€“Planck operator \( L = \nabla \cdot [D(X)\nabla V(X) - \nabla \cdot D(X)] \) is discretized on a periodic 1D torus.
- The largest negative eigenvalue \( \lambda_D \) (the **spectral gap**) determines the convergence rate.
- FEM matrices \( A_D \) and \( M_D \) are built from basis functions \( (\phi_i) \).

### ğŸ§­ Gradient Descent Optimization
We maximize the spectral gap \( |\lambda_D| \) via gradient descent on \( D(x) \), constrained by  
\[
\int_{T^1} D(q)^2 \, dq \leq 1
\]
to avoid trivial scaling.

### âš¡ Convergence Comparison
Two diffusions were compared:
- Constant \( D \equiv 1 \)
- Optimized \( D_{optimal}(x) \)

ğŸ“Š **Result:**  
The optimized diffusion leads to **faster mixing and convergence** (verified via total variation distance).

---

## ğŸ§ª Numerical Experiments

| Algorithm | Implementation | Key Result |
|------------|----------------|-------------|
| Rejection Sampling | Uniform & exponential targets | High rejection rate in high dimension |
| Metropolisâ€“Hastings | Gaussian & periodic potentials | Robust, converges to target |
| MALA | Gradient-informed diffusion | Faster convergence than MH |
| FEM Optimization | Langevin with variable D(x) | Optimized diffusion improves mixing speed |

Convergence was measured using **total variation distance** between the empirical and target distributions.

---

## ğŸ“ˆ Results Summary

- **FEM optimization** significantly improves sampling efficiency.  
- **MALA** outperforms MH in convergence speed.  
- **Markov chain theory** provides the rigorous justification of ergodic convergence.  
- Demonstrations confirm faster transitions and lower bias in optimized settings.

---

## ğŸ“š References

1. Hastings, W.K. *Monte Carlo Sampling Methods Using Markov Chains and Their Applications*, *Biometrika*, 1970.  
2. LeliÃ¨vre, T., Pavliotis, G.A., Nier, F. *Optimal Non-Reversible Linear Drift for the Convergence to Equilibrium of a Diffusion*, *J. Stat. Phys.*, 2012.  
3. Landau, D.P., Binder, K. *A Guide to Monte Carlo Simulations in Statistical Physics*, Cambridge Univ. Press, 2014.  
4. Robert, C. *MÃ©thodes de Monte Carlo par chaÃ®nes de Markov*, 1996.  
5. Reygner, J., LeliÃ¨vre, T. *MÃ©thodes NumÃ©riques Probabilistes*, CERMICS, 2022.  

---

## ğŸ§­ Conclusion

This project bridges **probability theory**, **numerical analysis**, and **scientific computing** by combining:

- **Theoretical rigor** (ergodic theorems, Markov chains)  
- **Algorithmic implementation** (MCMC and Langevin methods)  
- **Optimization techniques** (finite elements and spectral gap maximization)

It provides both an educational framework for understanding stochastic sampling and a foundation for **future research in high-dimensional MCMC acceleration**.

